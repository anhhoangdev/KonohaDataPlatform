apiVersion: v1
kind: ConfigMap
metadata:
  name: kyuubi-dbt-defaults
  namespace: kyuubi
  labels:
    app: kyuubi-dbt
    component: kyuubi-config
    part-of: data-platform
    managed-by: kustomize
data:
  kyuubi-defaults.conf: |-
    # =============== Kyuubi DBT (USER level) DEFAULTS ===============
    kyuubi.engine.share.level=USER
    kyuubi.session.engine.idle.timeout=PT30S
    kyuubi.ha.enabled=false

    # Front-end bind
    kyuubi.frontend.thrift.binary.bind.host=0.0.0.0
    kyuubi.frontend.rest.bind.host=0.0.0.0

    # Listener ports
    kyuubi.frontend.thrift.binary.bind.port=10009
    kyuubi.frontend.rest.bind.port=10099

    # Engine session housekeeping
    kyuubi.session.engine.check.interval=PT5S
    kyuubi.session.variable.substitution.enabled=true
    kyuubi.session.engine.spark.sql.adaptive.enabled=true

    # Spark pod templates & namespace defaults
    spark.kubernetes.driver.podTemplateFile=/opt/spark/pod-templates/driver-template.yaml
    spark.kubernetes.executor.podTemplateFile=/opt/spark/pod-templates/executor-template.yaml

    # Default Spark resources for DBT jobs (will be overridden by DBT operator)
    spark.executor.instances=1
    spark.executor.memory=1g    
    spark.executor.cores=1      
    spark.driver.memory=1g      
    spark.driver.cores=1        

    # DISABLE dynamic allocation to allow explicit executor control
    spark.dynamicAllocation.enabled=false
    
    # ===== CRITICAL: Allow DBT operator's pod naming and resource control =====
    # Disable Kyuubi's forced pod name rewriting completely
    kyuubi.kubernetes.spark.forciblyRewriteDriverPodName.enabled=false
    kyuubi.kubernetes.spark.forciblyRewriteExecutorPodNamePrefix.enabled=false
    
    # Allow ALL session configurations to be overridden (including pod names and resources)
    kyuubi.session.conf.restrict.list=
    kyuubi.session.conf.ignore.list=
    
    # Enable session-level Spark configuration inheritance
    kyuubi.session.engine.initialize.sql=
    kyuubi.session.engine.spark.initialize.sql=
    
    # Allow Spark configuration passthrough from connection properties
    kyuubi.session.engine.spark.conf.isolation=false

    # Fix for Ivy dependency resolution issues
    spark.jars.ivy=/tmp/.ivy2
    spark.sql.execution.arrow.pyspark.enabled=false
    spark.kubernetes.driver.deleteOnTermination=true
    spark.kubernetes.executor.deleteOnTermination=true
    spark.kubernetes.local.dirs.tmpfs=false

    # ================= Spark on Kubernetes defaults =================
    spark.master=k8s://https://kubernetes.default:443
    spark.submit.deployMode=cluster
    spark.kubernetes.namespace=kyuubi
    spark.kubernetes.container.image=spark-engine-iceberg:3.5.0-1.4.2
    spark.kubernetes.authenticate.driver.serviceAccountName=kyuubi-sa
    spark.kubernetes.file.upload.path=/tmp
    spark.local.dir=/tmp/spark-local

    # Pass S3 credentials to Spark driver and executor pods
    spark.kubernetes.driverEnv.AWS_ACCESS_KEY_ID=${SPARK_HADOOP_FS_S3A_ACCESS_KEY}
    spark.kubernetes.driverEnv.AWS_SECRET_ACCESS_KEY=${SPARK_HADOOP_FS_S3A_SECRET_KEY}
    spark.kubernetes.executorEnv.AWS_ACCESS_KEY_ID=${SPARK_HADOOP_FS_S3A_ACCESS_KEY}
    spark.kubernetes.executorEnv.AWS_SECRET_ACCESS_KEY=${SPARK_HADOOP_FS_S3A_SECRET_KEY}

    # Iceberg extensions & catalogs
    spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
    spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.spark_catalog.type=hive
    spark.sql.catalog.spark_catalog.uri=thrift://hive-metastore:9083
    spark.sql.catalog.spark_catalog.warehouse=s3a://warehouse/

    spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
    spark.sql.catalog.iceberg.type=hive
    spark.sql.catalog.iceberg.uri=thrift://hive-metastore:9083
    spark.sql.catalog.iceberg.warehouse=s3a://warehouse/

    # S3 endpoint (path-style)
    spark.hadoop.fs.s3a.endpoint=http://minio:9000
    spark.hadoop.fs.s3a.path.style.access=true
    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
    # S3 credentials - these will be populated from environment variables
    spark.hadoop.fs.s3a.access.key=${SPARK_HADOOP_FS_S3A_ACCESS_KEY}
    spark.hadoop.fs.s3a.secret.key=${SPARK_HADOOP_FS_S3A_SECRET_KEY}

    # Hive metastore & event logs
    spark.hive.metastore.uris=thrift://hive-metastore:9083
    spark.eventLog.dir=/tmp/spark-events

    # Local engine jar path
    kyuubi.session.engine.spark.main.resource=local:///opt/kyuubi/externals/engines/spark/kyuubi-spark-sql-engine.jar 